import os
import h5py
import numpy as np
import pickle
# from simba.annotation.annotator import SimAnnotator
import logging


"""
THis module handle the storage of all the probability generated by the mixed Sampling
at the moment this is stored in an HDF5. To avoid ocmputationnal expensive storage it is stored as fo;low
    * samples/links: (List of strings): The name of the links stored. Length (L).
    * samples/num_batches: (int) The number of batches currently stored
    * samples/num_by_batch: (int) The number of samples by batch
    * samples/num_features: (int) The number of features
Then batch are all stored in the 'data' group:
    * samples/feati/batchj: Store the probability of of the candidates of feature i and batch j. It is a numpy array of shape (num_by_batch x num candidates (feati) x L)
The prior is stored in the prior group:
    * priors/feati: Store the prior probablity of the candidates of feature i.  Numpy array of shape ( num candidates (feati) )
The probabilities are not normalized.
The clusters are stored in the cluster group:
    * clusters/clusti: The i eme cluster (features,candidates) is stored in this fields
We don t store the probabilities if there is a single candidates for space efficiceny
"""

#Constant names fields
GROUP_NAME = "samples_probs"
SAMPLE_NAME = "samples"
CLUST_NAME = "clusters"
PRIOR_NAME = "priors"
CANDIDATES_NAME = "candidates"
INFOS_NAME = "infos"
CANDIDATES_INFOS_NAME = INFOS_NAME+"/candidates"
FEATURES_INFOS_NAME = INFOS_NAME+"/features"
FEATURES_INFOS_NAME = INFOS_NAME+"/features_candidates"

#Id construction function for the hdf5
def make_feature_key(feat):
    return "f%i" % feat

def make_candidate_key(cand):
    return "c{:d}".format(cand)

def make_batch_key(batch):
    return "b%i" % batch

def make_clust_key(clust):
    return "c{:i}_feat".format(clust)

def make_sample_key(sample):
    "s{:i}".format(sample)

class HDF5storer:
    def __init__(self,filename,buffer_size = 50):
        self.filename = filename
        if os.path.isfile(filename):
            raise OSError("'"+filename+"' already exists, please remove it.")

        self.f = h5py.File(filename, "w")

        self.links = []
        self.buffer_size = buffer_size
        self.num_by_batch = buffer_size
        self.buffers = {}
        self.num_cand_features = []
        self.buffers_samples = []
        self.prob_unknown_prior = 0.0
        self.prob_unknown_link = 0.0
        self.num_features = 0
        self.buffer_pos = 0
        self.batch_count = 0
        self.total_sample = 0


    def initialize_fieds(self,features,links,GA):
        """Call it once at the beginning to initialize the buffers and everything."""
        # if not isinstance(annot,SimAnnotator):
        #     raise ValueError("annot should be a SimAnnotator object")
        self.num_features = len(features)
        samples = self.f.create_group(SAMPLE_NAME)
        samples.create_dataset(SAMPLE_NAME,shape=(self.num_features,0),maxshape=(self.num_features,None),chunks=True)

        # Initialize links
        for link in links:
            # TODO: Should be a getter.
            lname = link.__class__.__name__
            self.links.append(lname)
        L = len(self.links)

        # Num features
        self.num_cand_features = [len(GA[ff])-1 for ff in range(self.num_features)]

        # Annot candidates
        for ifeat in range(self.num_features):
            if self.num_cand_features[ifeat]==0:
                continue
            else:
                target_shape = (self.num_by_batch,self.num_cand_features[ifeat],L)
                self.buffers[ifeat] = np.zeros(target_shape)
        self.store_candidates(GA)


    def initalize_buffer_from_clusters(self,clusters):
        L = len(self.links)
        for feats,cands in clusters:
            ncands = len(cands)
            for ifeat in feats:
                self.num_cand_features[ifeat] = ncands
                target_shape = (self.num_by_batch,ncands,L)
                # Annot candidates
                if self.num_cand_features[ifeat]==0:
                    continue
                else:
                    self.buffers[ifeat] = np.zeros(target_shape)


    def set_unknown_prob(self,prior,plink):
        self.prob_unknown_prior = prior
        self.prob_unknown_link = plink

    def reset_buffer(self):
        """Call it after each storage to rest the dataset."""
        L = len(self.links)
        for ifeat in range(self.num_features):
            if self.num_cand_features[ifeat]<=1:
                continue
            else:
                target_shape = (self.num_by_batch,self.num_cand_features[ifeat],L)
                self.buffers[ifeat] = np.zeros(target_shape)
        self.buffers_samples = []
        self.buffer_pos = 0

    def store_prior(self,GA):
        """Store the prior probablity"""
        prior = self.f.create_group(PRIOR_NAME)
        for ifeat in range(self.num_features):
            prior_values = [GA[ifeat][icand]["prior"] for icand in GA[ifeat]]
            fkey = make_feature_key(ifeat)
            prior.create_dataset(fkey,data=prior_values)

    def store_candidates(self,GA):
        """Store the candidate corresponding to each features"""
        cands = self.f.create_group(CANDIDATES_NAME)
        for ifeat in range(self.num_features):
            ccands = [icand-self.num_features for icand in GA[ifeat]]
            fkey = make_feature_key(ifeat)
            cands.create_dataset(fkey,data=ccands)


    def store_prob(self,feat,prob_mat):
        """Sotre links probability for a feature"""
        self.buffers[feat][self.buffer_pos,:,:] = prob_mat

    def store_sample(self,sample):
        self.buffers_samples.append(sample)

    def save_samples(self):
        """This function save samples in a signle array."""
        if len(self.buffers_samples)!=0:
            temp_samples = np.stack(self.buffers_samples,axis=1)
            snsn = SAMPLE_NAME+"/"+SAMPLE_NAME
            self.f[snsn].resize((self.f[snsn].shape[1] + temp_samples.shape[1]), axis = 1)
            self.f[snsn][:,-temp_samples.shape[1]:] = temp_samples


    def store_clusters(self,clusters):
        clusters = self.f.create_group(CLUST_NAME)
        for iclust,clust in enumerate(clusters):
            ckey = make_clust_key(iclust)
            clusters.create_dataset(ckey,clust)

    def next_iteration(self):
        """Iterate the batch counter and save if necessary"""
        self.buffer_pos += 1
        self.total_sample += 1
        if self.buffer_pos == self.buffer_size:
            if self.batch_count==0:
                self.write_header()
            self.save_batch()
            self.save_samples()
            #Reinitializing
            self.reset_buffer()


    def initialize_hdf5(self):
        """Write the inital information, links,sample by batch, number of batches"""
        self.f.create_group(GROUP_NAME)

    def write_header(self):
        """Write the header as attributes"""
        logging.info("Initializing header hdf5")
        self.f.create_group(GROUP_NAME)
        samp = self.f[GROUP_NAME]
        samp.attrs['links'] = self.links
        samp.attrs['num_batches'] = 0
        samp.attrs['num_by_batch'] = self.buffer_size
        samp.attrs['num_features'] = self.num_features
        samp.attrs['prob_unknown_prior'] = self.prob_unknown_prior
        samp.attrs['prob_unknown_link'] = self.prob_unknown_link
        #We also create all the features
        for ifeat in range(self.num_features):
            key_feat = make_feature_key(ifeat)
            samp.create_group(key_feat)

    def save_batch(self):
        """Save the current batch in the hdf5"""
        samp = self.f[GROUP_NAME]
        for ifeat in range(self.num_features):
            if ifeat not in self.buffers:
                continue
            key_feat = make_feature_key(ifeat)
            # cgroup = samp[key_feat]
            bkey = make_batch_key(self.batch_count)
            full_path = GROUP_NAME+"/"+key_feat+"/"+bkey
            self.f.create_dataset(full_path,data=self.buffers[ifeat])
        self.batch_count += 1
        samp = self.f[GROUP_NAME]
        samp.attrs['num_batches'] = self.batch_count

    def store_final_batch(self):
        """Store the last batch"""
        samp = self.f[GROUP_NAME]
        if self.buffer_pos!=0:
            for ifeat in range(self.num_features):
                key_feat = make_feature_key(ifeat)
                if ifeat not in self.buffers:
                    continue
                cgroup = samp[key_feat]
                bkey = make_batch_key(self.batch_count)
                full_path = GROUP_NAME+"/"+key_feat+"/"+bkey
                self.f.create_dataset(full_path,data=self.buffers[ifeat][:self.buffer_pos,:,:])

    def finish_sampling(self):
        """Store the last batch"""
        self.save_samples()
        self.store_final_batch()
        self.f.close()

    def __str__(self):
        return "HDF5storer object storing samples in "+self.filename


def _store_pickle(path,object):
    with open(path,"wb") as f:
        pickle.dump(object,file=f)

##This could be classes if necessary, but there is no reason
def store_candidates(path,candidates):
    _store_pickle(path,candidates)

def store_features(path,features):
    _store_pickle(path,features)
