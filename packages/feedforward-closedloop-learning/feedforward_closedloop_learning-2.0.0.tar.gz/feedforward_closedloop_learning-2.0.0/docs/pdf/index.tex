\href{https://journals.sagepub.com/doi/10.1177/1059712319851070}{\texttt{ Forward propagation closed loop learning Bernd Porr, Paul Miller. Adaptive Behaviour 2019.}}

\href{https://www.berndporr.me.uk/Porr_Miller_FCL_2019_Adaptive_Behaviour.pdf}{\texttt{ Submission version}}

For an autonomous agent, the inputs are the sensory data that inform the agent of the state of the world, and the outputs are their actions, which act on the world and consequently produce new sensory inputs. The agent only knows of its own actions via their effect on future inputs; therefore desired states, and error signals, are most naturally defined in terms of the inputs. Most machine learning algorithms, however, operate in terms of desired outputs. For example, backpropagation takes target output values and propagates the corresponding error backwards through the network in order to change the weights. In closed loop settings, it is far more obvious how to define desired sensory inputs than desired actions, however. To train a deep network using errors defined in the input space would call for an algorithm that can propagate those errors forwards through the network, from input layer to output layer, in much the same way that activations are propagated.

\href{https://github.com/glasgowneuro/feedforward_closedloop_learning}{\texttt{ Github project page}} 