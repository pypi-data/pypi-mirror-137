# Copyright (c) 2021 LightningV1p3r

####################
# Libs
####################

import viperlogger
from . import errors

####################
# Logger
####################

lexer_logger = viperlogger.Logger("Lexer", './logs/console-engine.log')
# lexer_logger.enable_debugmode('True')

####################
# Constants
####################

LETTERS = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
DIGITS = "0123456789"

####################
# Tokens
####################

TT_STR = "STR"
TT_INT = "INT"
TT_DOT = "DOT"
TT_EOF = "EOF"
TT_PLUS = "PLUS"
TT_BANG = "BANG"
TT_FILE = "FILE"
TT_FlAG = "FLAG"
TT_PATH = "PATH"
TT_SLASH = "SLASH"
TT_FLOAT = "FLOAT"
TT_MINUS = "MINUS"
TT_SHARP = "SHARP"
TT_QUOTE = "QUOTE"
TT_IPADDR = "IPADDR"
TT_ASTERISK = "ASTERISK"
TT_AMPERSAND = "AMPERSAND"
TT_UNDERSCORE = "UNDERSCORE"
TT_DOUBLE_QUOTE = "DOUBLE_QUOTE"
TT_FILE_EXTENSION = "FILE_EXTENSION"


class Token:

    """Token object generated by lexer.

    Returns:
        Token Type: Type of the Token.
        Value: Value of the token."""

    def __init__(self, type_, value=None) -> None:
        self.type = type_
        self.value = value

    def __repr__(self) -> str:
        return f"{self.type}: {self.value}" if self.value else f"{self.type}"


####################
# Lexer
####################


class Lexer:

    """Iterates through given Input and splits it up into Tokens."""

    def __init__(self, text) -> None:

        self.text = text
        lexer_logger.debug(f"To tokenize: '{self.text}'")
        self.pos = -1
        self.current_char = None
        self.tokens = []
        self.advance()

    ####################
    # Assets
    ####################

    def advance(self, iterations=1) -> None:

        """Advances the cursor by one char in the input."""

        iter_count = 0

        while iter_count < iterations:

            self.pos += 1
            self.current_char = (
                self.text[self.pos] if self.pos < len(self.text) else None
            )
            iter_count += 1
        lexer_logger.debug(f"advanced to pos {self.pos} with value: '{self.current_char}'")

    def reverse(self, iterations=1) -> None:

        """Reverses the cursor by one char in the input."""

        iter_count = 0

        while iter_count < iterations:

            self.pos -= 1

            if self.pos < 0:
                e = errors.InvalidCursorPosition(self.pos)
                lexer_logger.error(e.exception_msg, '')
                raise e
            else:
                self.current_char = self.text[self.pos]
                lexer_logger.debug(f"reversed to pos {self.pos} with value: '{self.current_char}'")

            iter_count += 1

    def merge_tokens(self, tok1, tok2, res_tok_type) -> Token:

        try:
            val1 = tok1.value
            val2 = tok2.value

            if val1 == int() or float() and val2 == int() or float():
                res_value = val1 + val2
            else:
                res_value = str(val1) + str(val2)

            lexer_logger.debug(f"Merged Tokens '{tok1}', '{tok2}' to Token with value: '{res_value}'")
            return Token(res_tok_type, res_value)

        except Exception as exception:
            e = errors.TokenMergingError(tok1, tok2, exception)
            lexer_logger.error(e.exception_msg, '')
            raise e

    def tokenize(self) -> list:

        """Starts the Tokenizing Process.

        Returns:
            List of Tokens
        """

        while self.current_char is not None:

            if self.current_char in " \t":
                self.advance()
            elif self.current_char in LETTERS:
                self.tokens.append(self.make_string())
            elif self.current_char in DIGITS:
                self.tokens.append(self.handle_digit())
            elif self.current_char == "-":
                self.tokens.append(self.handle_minus())
            elif self.current_char == "+":
                self.tokens.append(Token(TT_PLUS))
                self.advance()
            elif self.current_char == "/":
                self.tokens.append(self.handle_slash())
            elif self.current_char == "*":
                self.tokens.append(Token(TT_ASTERISK))
                self.advance()
            elif self.current_char == ".":
                self.tokens.append(self.handle_dot())
            elif self.current_char == "!":
                self.tokens.append(Token(TT_BANG))
                self.advance()
            elif self.current_char == "#":
                self.tokens.append(Token(TT_SHARP))
                self.advance()
            elif self.current_char == "'":
                self.tokens.append(Token(TT_QUOTE))
                self.advance()
            elif self.current_char == "&":
                self.tokens.append(Token(TT_AMPERSAND))
                self.advance()
            elif self.current_char == "_":
                self.tokens.append(Token(TT_UNDERSCORE))
                self.advance()
            elif self.current_char == '"':
                self.tokens.append(Token(TT_DOUBLE_QUOTE))
                self.advance()
            else:
                e = errors.IllegalCharError(self.current_char)
                lexer_logger.critical(e.exception_msg, '')
                raise e

        self.tokens.append(Token(TT_EOF))

        return self.tokens

    ####################
    # Checker
    ####################

    def checkforip(self) -> bool:  # sourcery skip: class-extract-method

        """Checks for possible IP addr"""

        iterations = 0
        dot_count = 0

        while self.current_char is not None and self.current_char in DIGITS + ".":

            if self.current_char == ".":
                if dot_count == 3:
                    break
                dot_count += 1

            iterations += 1
            self.advance()

        self.reverse(iterations)
        log_msg = f'Found IP Address @{self.pos}' if dot_count > 1 else 'No IP Found!'
        lexer_logger.debug(log_msg)
        return dot_count > 1

    def checkforpath(self) -> bool:

        """Checks for pssible Path."""

        iterations = 0
        slash_count = 0

        while (
            self.current_char is not None
            and self.current_char in LETTERS + DIGITS + "/"
        ):

            if self.current_char == "/":
                if slash_count == 2:
                    break
                slash_count += 1

            self.advance()
            iterations += 1

        self.reverse(iterations)
        log_msg = f'Found path @{self.pos}' if slash_count > 1 else 'No path found!'
        lexer_logger.debug(log_msg)
        return slash_count > 1

    def checkforfile(self) -> bool:

        iterations = -1
        file_found = bool()

        while (
            self.current_char is not None
            and self.current_char in LETTERS + DIGITS + "."
        ):

            if self.current_char in " \t":
                break

            elif self.current_char in DIGITS:
                file_found = False
                break
            else:
                file_found = True

            self.advance()
            iterations += 1

        self.reverse(iterations + 1)
        log_msg = f'Found file @{self.pos}' if file_found and iterations >= 1 else 'No file found!'
        lexer_logger.debug(log_msg)
        return file_found and iterations >= 1

    ####################
    # handlers
    ####################

    def handle_digit(self) -> Token:
        return self.make_ipaddr() if (res := self.checkforip()) else self.make_number()

    def handle_minus(self) -> Token:

        self.advance()
        if self.current_char != None and self.current_char not in " \t":
            self.advance()
            if self.current_char is None or self.current_char in " \t":
                self.reverse()
                return self.make_flag()
        else:
            self.reverse()
            self.advance()
            return Token(TT_MINUS)

    def handle_slash(self) -> Token:

        res = self.checkforpath()
        if res is True:
            return self.make_path()

        self.advance()
        return Token(TT_SLASH)

    def handle_dot(self):

        res = self.checkforfile()
        if res is True:
            self.tokens.append(self.make_file_extension())
            return self.make_file()
        else:
            self.advance()
            return Token(TT_DOT)

    ####################
    # Token generators
    ####################

    def make_string(self) -> Token:

        """Generates a Token of Type String."""

        string = ""

        while self.current_char is not None and self.current_char in LETTERS:

            string += self.current_char
            self.advance()

        lexer_logger.debug(f"Generated TT_STR with value: '{string}'")
        return Token(TT_STR, string)

    def make_number(self) -> Token:

        """Generates Token of Type Integer."""

        num = ""
        dot_count = 0
        is_file = False

        while self.current_char is not None and self.current_char in DIGITS + ".":

            if self.current_char == ".":
                if dot_count == 1:
                    break
                file_res = self.checkforfile()

                if file_res is False:
                    dot_count += 1
                    num += "."
                    self.advance()
                else:
                    is_file = True
                    break

            else:
                num += self.current_char
                self.advance()
        if is_file:
            self.tokens.append(Token(TT_INT, int(num)))
            self.tokens.append(self.make_file_extension())
            return self.make_file()
        elif dot_count == 0:
            lexer_logger.debug(f"Generated TT_INT with value: '{num}'")
            return Token(TT_INT, int(num))
        else:
            lexer_logger.debug(f"Generated TT_FLOAT with value: '{num}'")
            return Token(TT_FLOAT, float(num))

    def make_ipaddr(self) -> Token:

        """Generates Token of Type IP Addr."""

        ip = ""
        dot_count = 0

        while self.current_char is not None and self.current_char in DIGITS + ".":

            if self.current_char == ".":
                if dot_count == 3:
                    break
                dot_count += 1
                ip += "."
            else:
                ip += self.current_char

            self.advance()

        lexer_logger.debug(f"Generated TT_IPADDR with value: '{ip}'")
        return Token(TT_IPADDR, ip)

    def make_flag(self) -> Token:

        """Generates Token of Type Flag."""

        flag = ""

        while self.current_char is not None and self.current_char in LETTERS:

            if self.current_char in " \t":
                break

            flag += self.current_char
            self.advance()

        lexer_logger.debug(f"Created TT_FLAG with value: '{flag}'")
        return Token(TT_FlAG, flag)

    def make_path(self) -> Token:

        """Generates Token of Type Path."""

        path = ""

        while (
            self.current_char is not None
            and self.current_char in LETTERS + DIGITS + "/"
        ):

            path += self.current_char
            self.advance()

            if self.current_char == ".":

                iterations = 0

                while True:

                    if self.current_char == "/":
                        iterations -= 1
                        self.advance()
                        break

                    self.reverse()
                    iterations += 1

                path = path[:-iterations]
                break

        lexer_logger.debug(f"Created TT_PATH with value: '{path}'")
        return Token(TT_PATH, path)

    def make_file(self) -> Token:

        try:
            filename = self.tokens[-2]
            file_ext = self.tokens[-1]
        except IndexError:
            filename = self.tokens[1]
            file_ext = self.tokens[2]

        file_ = self.merge_tokens(filename, file_ext, TT_FILE)

        try:
            del self.tokens[-1]
            del self.tokens[-1]
        except IndexError:
            del self.tokens[-1]

        lexer_logger.debug(f"Created TT_FILE with value: '{file_.value}'")
        return file_

    def make_file_extension(self) -> Token:

        file_extension = ""

        while (
            self.current_char is not None
            and self.current_char in LETTERS + DIGITS + "."
        ):

            if self.current_char in " \t":
                break

            file_extension += self.current_char
            self.advance()

        lexer_logger.debug(f"Created TT_FILE_EXTENSION with value: '{file_extension}'")
        return Token(TT_FILE_EXTENSION, file_extension)
