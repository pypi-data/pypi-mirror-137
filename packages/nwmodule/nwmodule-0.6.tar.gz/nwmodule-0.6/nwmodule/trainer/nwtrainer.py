from abc import ABC, abstractmethod
import torch as tr
from pathlib import Path
from typing import Dict, Any, Iterable, Iterator, Optional
from datetime import datetime
from nwutils.others import RunningMean
from torchmetrics import Metric
from nwutils.torch import trGetData, trToDevice, getOptimizerStr
from ..nwmodule import NWModule
from ..logger import logger, drange

# Generic module that takes a NWModule and trains it on some train/validation iterator
# TODO: Would be nice if we could work with a nn.Module directly in future and ignore some missing features
class NWTrainer(ABC):
    # @param[in] model The NWModule that is required for the training.
    def __init__(self, model:NWModule, workingDirectory:Path=None):
        self.model = model
        if workingDirectory is None:
            workingDirectory = Path.cwd()
        self.workingDirectory = workingDirectory
        self.workingDirectory.mkdir(exist_ok=True, parents=True)

    @abstractmethod
    def train_step(self, x, gt, is_optimizing: bool):
        pass

    def initializeEpochMetrics(self):
        metrics = self.model.getMetrics()
        res = {metric.name: RunningMean(initValue=metric.defaultValue()) for metric in metrics}
        return res

    # Other neural network architectures can update these
    def callbacksOnEpochStart(self):
        # Call onEpochStart here, using only basic args
        for callback in self.model.getCallbacks():
            callback.on_epoch_start(model=self.model, working_directory=self.workingDirectory)

    def epochPrologue(self, epochResults):
        res = {}
        metrics = self.model.getMetrics()
        # First, turn metrics from running means to actual numbers
        for Key in epochResults.keys():
            res[Key] = {}
            for epochKey, epochValue in epochResults[Key].items():
                if epochKey in metrics:
                    epochValue = epochValue.get()
                res[Key][epochKey] = epochValue

        if self.model.getOptimizer() is not None:
            optimizerStr = getOptimizerStr(self.model.getOptimizer())
            res["Optimizer"] = optimizerStr
        self.model.getTrainHistory().append(res)

        for callback in self.model.getCallbacks() + self.model.getMetrics():
            callback.on_epoch_end(model=self.model, epoch_results=res, working_directory=self.workingDirectory)
        return res

    def callbacksOnIterationStart(self):
        for callback in self.model.getCallbacks():
            callback.on_iteration_start()

    def callbacksOnIterationEnd(self, x, gt, y, loss, iteration, numIterations, metricResults, prefix):
        iterResults = {}
        modelMetrics = self.model.getMetrics()
        for callback in self.model.getCallbacks() + self.model.getMetrics():
            callback_kwargs = {"model": self.model, "data": x, "loss": loss, "iteration": iteration, "prefix": prefix,
                "numIterations": numIterations, "iterResults": iterResults, "metricResults": metricResults}
            callbackResult = callback.on_iteration_end(y, gt, **callback_kwargs)
            callbackResult = callback.iteration_reduce_function(callbackResult)
            iterResults[callback] = callbackResult

            # Add it to running mean only if it's numeric. Here's why the metrics differ for different batch size
            #  values. There's no way for us to infer the batch of each iteration, so we assume it's 1.
            if callback in modelMetrics:
                assert callback.name in metricResults, f"Metric {callback.name} not in metric results"
                metricResults[callback.name].update(callbackResult, count=1)
        return metricResults

    @staticmethod
    def do_optimizer_step(model: NWModule, loss: float):
        model.getOptimizer().zero_grad()
        loss.backward(retain_graph=False)
        model.getOptimizer().step()

    # Basic method that does a forward phase for one epoch given a iterator. It can apply a step of optimizer or not
    # @param[in] Iterator Object used to get a batch of data and labels at each step
    # @param[in] stepsPerEpoch How many items to be generated by the iterator
    # @param[in] prefix The prefix for the tqdm range and the key of the resulting dictionary
    # @return A dictionary with the metrics over all the epoch steps and the duration
    def runOneEpoch(self, iterator:Iterator, stepsPerEpoch:int, \
            is_optimizing:bool, prefix:str="", globalMetrics:bool=False) -> Dict[str, float]:
        assert stepsPerEpoch > 0
        if is_optimizing == False and tr.is_grad_enabled():
            logger.info("Warning! Not optimizing, but grad is enabled.")
        # This variable holds all the returned items of this epoch. We'll store metrics (by name) and the duration it
        #  took for the epoch to complete.
        res = self.initializeEpochMetrics()

        startTime = datetime.now()
        Range = drange(stepsPerEpoch, desc=f"[{prefix}] Iteration", postfix={str(k):0.000 for k in res})
        for i in Range:
            npItems = next(iterator)
            # Required for multi-GPU setup.
            items = trToDevice(trGetData(npItems), self.model.getDevice())
            # TODO: we still assume inputs, labels = data. Update this to be generic!
            x, gt = items["data"], items["labels"]

            self.callbacksOnIterationStart()
            y, loss = self.train_step(x, gt, is_optimizing=is_optimizing)
            self.callbacksOnIterationEnd(x, gt, y, loss, i, stepsPerEpoch, res, prefix)

            # For performance reasons we update the shown loss less often.
            if isinstance(Range, drange) and (i % 30 == 0 or i == stepsPerEpoch - 1):
                # TODO: some architectures (i.e. graph) have deep results (so dict[dict[running_mean]])
                currentResults = {}
                for k in res:
                    if not isinstance(res[k], RunningMean):
                        continue

                    currentResults[str(k)] = f"{res[k].get():2.3f}"
                Range.set_postfix(currentResults)

        res["duration"] = datetime.now() - startTime
        return res

    def train(self, reader: Iterable, num_epochs: int, validation_reader: Optional[Iterable]=None, \
              num_steps: Optional[int]=None, validation_num_steps: Optional[int]=None):
        current_epoch = len(self.model.getTrainHistory()) + 1
        assert current_epoch <= num_epochs + 1, f"{current_epoch} vs {num_epochs}"

        num_epochs_left = num_epochs - current_epoch + 1
        if num_epochs_left == 0:
            logger.info(f"Model is already trained for {current_epoch} epochs. Skipping.")
            return

        num_steps = num_steps if not num_steps is None else len(reader)
        validation_num_steps = None if validation_reader is None else validation_num_steps if not \
            validation_num_steps is None else len(validation_reader)
        logger.info(f"Training for {num_epochs_left} epochs starting from epoch {current_epoch}.")
        logger.info(f"Train set steps per epoch: {num_steps}")
        if not validation_reader is None:
            logger.info(f"Using validation set. Num steps per epoch: {validation_num_steps}")

        self.model.num_epochs = num_epochs
        self.model.finished = False
        for _ in drange(num_epochs_left, initial=current_epoch, total=num_epochs, position=0, desc="Epoch"):
            if self.model.finished:
                logger.info("Finished flag was set from somewhere else. Stopping!")
                break

            self.callbacksOnEpochStart()
            epochResults = {}
            # Run for training data and append the results
            # No iteration callbacks are used if there is a validation set (so iteration callbacks are only
            #  done on validation set). If no validation set is used, the iteration callbacks are used on train set.
            res = self.runOneEpoch(iter(reader), num_steps, is_optimizing=True, prefix="Train")
            epochResults["Train"] = res

            # Run for validation data and append the results
            if not validation_reader is None:
                self.model.eval()
                self.callbacksOnEpochStart()
                with tr.no_grad():
                    _iter = iter(validation_reader)
                    res = self.runOneEpoch(_iter, validation_num_steps, is_optimizing=False, prefix="Validation")
                epochResults["Validation"] = res
                self.model.train()

            self.epochPrologue(epochResults=epochResults)
            if not self.model.optimizerScheduler is None:
                self.model.optimizerScheduler.step()

    def test(self, reader, num_steps:Optional[int]=None):
        num_steps = num_steps if not num_steps is None else len(reader)
        with tr.no_grad():
            self.callbacksOnEpochStart()
            res = self.runOneEpoch(iter(reader), num_steps, is_optimizing=False, prefix="Test")
            res = self.epochPrologue({"Test": res})
        return res

    def __call__(self, *args, **kwargs):
        self.train(*args, **kwargs)
