import csv
import os
import time

import moolib
import gym
import torch
from torch import nn
from torch.nn import functional as F

ADDRESS = "127.0.0.1:5541"
ACT_BATCH_SIZE = 1
ROLLOUT_LENGTH = 64

DISCOUNT = 0.99
LR = 1e-3
BASELINE_COST = 0.5
ENTROPY_COST = 0.0006


class Model(nn.Module):
    def __init__(self, num_actions=2):
        super(Model, self).__init__()

        self.linear0 = nn.Linear(4, 128)
        self.linear1 = nn.Linear(128, 32)

        self.policy = nn.Linear(32, num_actions)
        self.baseline = nn.Linear(32, 1)

    def forward(self, observation, unroll=False):
        if not unroll:
            # [B, ...] -> [T=1, B, ...].
            observation = observation.unsqueeze(0)

        T, B, *_ = observation.shape
        x = observation.reshape(T * B, -1)

        x = F.relu(self.linear0(x))
        x = F.relu(self.linear1(x))

        core_output = x

        policy_logits = self.policy(core_output)
        baseline = self.baseline(core_output)

        action = torch.multinomial(
            F.softmax(policy_logits, dim=1), num_samples=1
        )

        action = action.view(T, B)
        policy_logits = policy_logits.view(T, B, -1)

        output = (action, policy_logits, baseline)
        if not unroll:
            for t in output:
                t.squeeze_(0)
        return output


def create_env():
    return gym.make("CartPole-v0")


def a2c_loss(state, action, reward, done, model):
    # This time with gradients.
    unused_actions, policy_logits, baseline = model(state, unroll=True)

    reward = reward[1:]
    done = done[1:]
    policy_logits = policy_logits[:-1]
    bootstrap_value = baseline[-1]
    baseline = baseline[:-1]
    action = action[:-1]

    T, B, *_ = reward.shape

    discount = ~done * DISCOUNT
    returns = torch.empty(T, B)

    acc = bootstrap_value.detach()
    for t in range(T - 1, -1, -1):
        acc = reward[t] + acc * discount[t]
        returns[t] = acc

    advantages = returns - baseline

    policy = F.softmax(policy_logits, dim=-1)
    log_policy = F.log_softmax(policy_logits, dim=-1)

    log_pi = torch.gather(log_policy, dim=-1, index=action[..., None])
    log_pi.squeeze_(-1)

    # Alternative:
    # cross_entropy = F.nll_loss(
    #     F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),
    #     target=torch.flatten(actions, 0, 1),
    #     reduction="none",
    # )
    # cross_entropy = cross_entropy.view_as(returns)

    pg_loss = -torch.sum(log_pi * advantages.detach()) / B
    baseline_loss = 0.5 * torch.sum(advantages ** 2) / B
    entropy_loss = torch.sum(policy * log_policy) / B

    return pg_loss + BASELINE_COST * baseline_loss + ENTROPY_COST * entropy_loss


def main():
    # EnvPool runs a batch of environments in separate processes,
    # create_env is a user-defined function that returns a gym environment.
    env_pool = moolib.EnvPool(create_env, 1)

    model = Model()

    # TODO: This seems more complicated than necessary for this simple case?
    broker_rpc = moolib.Rpc()
    broker_rpc.set_name("broker")
    broker = moolib.Broker(broker_rpc)
    broker_rpc.listen(ADDRESS)
    peer = moolib.Rpc()
    group = moolib.Group(peer, "broker", "test group", 10)
    accumulator = moolib.Accumulator(
        group, "foo", model.parameters(), model.buffers()
    )
    peer.connect(ADDRESS)

    # Our optimizer.
    opt = torch.optim.Adam(model.parameters(), lr=LR)

    envs = env_pool.spawn()

    states = []
    actions = []
    rewards = []
    dones = []
    episode_returns = []

    action_t = torch.zeros(ACT_BATCH_SIZE, dtype=torch.int64)
    episode_step_t = torch.zeros(ACT_BATCH_SIZE, dtype=torch.int64)
    episode_return_t = torch.zeros(ACT_BATCH_SIZE)

    local_loss_computes = 0
    local_update_steps = 0

    while True:
        broker.update()

        if accumulator.connected():
            # TODO: This should start with the initial state, not the model.
            obs_future = envs.step(0, action_t)
            obs = obs_future.result()

            episode_step_t += 1
            episode_return_t += obs["reward"]

            state_t = obs["state"].to(torch.float, copy=True)
            with torch.no_grad():
                action_t, logits_t, _ = model(state_t)

            episode_returns.append(episode_return_t.clone())
            episode_step_t *= ~obs["done"]
            episode_return_t *= ~obs["done"]

            states.append(state_t)
            actions.append(action_t)
            rewards.append(obs["reward"].clone())
            dones.append(obs["done"].clone())

        else:
            # If we're not connected, sleep for a bit so we don't busy-wait
            print("Your training will commence shortly.")
            time.sleep(0.25)

        # update does some internal moolib book-keeping,
        # makes sure we're still connected, etc.
        # TODO: Why do we have 3 things to update lol?
        group.update()
        accumulator.update()

        if accumulator.wants_state():
            # For sharing the initial optimizer state with new nodes.
            accumulator.set_state({"optimizer": opt.state_dict()})

        if accumulator.has_new_state():
            # This node wants an initial optimizer state.
            opt.load_state_dict(accumulator.state()["optimizer"])

        if accumulator.wants_gradients():
            if len(states) < ROLLOUT_LENGTH + 1:
                # No data to train on, so this node won't participate in the
                # gradient reduction this time. Gradients may still be reduced
                # from the other nodes.
                accumulator.skip_gradients()
            else:
                # Train step: We have data to train on. Calculate
                # a local gradient and initiate a reduction.
                state = torch.stack(states)
                action = torch.stack(actions)
                reward = torch.stack(rewards)
                done = torch.stack(dones)

                loss = a2c_loss(state, action, reward, done, model)

                # returns = torch.stack(episode_returns)
                # print(done, returns)
                mean_episode_return = None
                if done.any():
                    returns = torch.stack(episode_returns)
                    mean_episode_return = "%f" % torch.mean(returns[done])
                    # print("mean_episode_return", mean_episode_return)

                del states[:-1]
                del actions[:-1]
                del rewards[:-1]
                del dones[:-1]
                del episode_returns[:-1]

                loss.backward()
                local_loss_computes += 1

                agent_steps = (
                    local_loss_computes * ROLLOUT_LENGTH * ACT_BATCH_SIZE
                )

                if mean_episode_return is not None:
                    log_to_file(
                        step=agent_steps,
                        loss=loss.item(),
                        mean_episode_return=mean_episode_return,
                    )

                if local_loss_computes % 100 == 0:

                    print(
                        "Local loss computation %d. (%d agent steps) "
                        "Mean episode return: %s (%d episode ends). "
                        "Local update steps: %d"
                        % (
                            local_loss_computes,
                            agent_steps,
                            mean_episode_return,
                            torch.sum(done),
                            local_update_steps,
                        )
                    )

                # Trigger an asynchronous gradient reduction.
                # has_gradients() will return true when the reduction is done
                accumulator.reduce_gradients(ACT_BATCH_SIZE)

        if accumulator.has_gradients():
            # moolib has reduced gradients, so we can step the optimizer
            opt.step()
            accumulator.zero_gradients()  # has_gradients() will return false after this
            local_update_steps += 1

        # While we are waiting for gradients, we can't do any more train
        # steps but we can still run inference to generate more data.
        # That will happen in the next iteration.


if __name__ == "__main__":
    main()
