import numpy as np
from numpy.linalg import norm
from numpy import zeros, sum, abs, min



def PenCF( obj_fun,  manifold, Xinit = None, beta = None, penc_radius = 1.01, maxit= 100, gtol = 1e-5, post_process = True, verbosity = 2, **kwargs):
    """
    First-order method for optimization problem
    over the Stiefel manifoldm, derived from from the exact penalty model with compact and convex constraint 

    Arguments: 
    * obj_fun: callable
        The objective function, which should be called by ``fval, grad = obj_fun(X)`` 
    
    * manifold: STOP manifold class
        The manifold class provides essential functions for the solvers that could be defined by 
            from pystop.manifold import Stiefel 
            manifold = Stiefel(1000,10) 

    * Xinit: numpy ndarray in shape (n,p)
        optional, Starting point of the solver (better if it is on the manifold). 
        If none then a starting point will be generated by manifold.Init_point().


    Outputs:
        X: numpy ndarray in shape (n,p)
            The final results of the solver
        output_dict: dict 
            The dictionary that contains essential informations. 
    """
    kkts = []
    feas = []
    fvals = []

    if Xinit is None:
        Xinit = manifold.Init_point()



    n = manifold._n
    p = manifold._p

    X = Xinit

    X_p = zeros( (n,p) )

    fval, gradf = obj_fun(X)


    if beta is None:
        beta = 0.1*norm(gradf, 'fro')

    gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))
    L = norm(gradf,'fro') + norm(gradr,'fro')
    feasibility = manifold.Feas_eval(X)
   


    for jj in range(maxit):

        if jj < 3:
            stepsize = 0.01/L
        else:
            stepsize = abs( sum( S * Y ) / sum( Y* Y ) )
            stepsize = min( (stepsize, 1e10) )

        X_p = X

        X = X - stepsize * gradr

        X = penc_safeguard(X, penc_radius, feasibility)

        S = X - X_p

        fval, gradf = obj_fun(X)
        gradr_p = gradr
        gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))
        Y = gradr - gradr_p

        substationarity = norm(gradr, 'fro')
        feasibility = manifold.Feas_eval(X)

        if verbosity == 2 and np.mod(jj,20) == 0:
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

        kkts.append( substationarity )
        feas.append( feasibility )
        fvals.append( fval )



        if substationarity < gtol:
            if verbosity >= 1:
                print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

            break

    if post_process:
        X = manifold.Post_process(X)
        fval, gradf = obj_fun(X)
        gradr = manifold.JA(X, gradf)
        substationarity = norm(gradr, 'fro')
        feasibility = manifold.Feas_eval(X)

        if verbosity >= 1:
            print("Post-processing")
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))


        kkts[-1] = substationarity
        feas[-1] = feasibility
        fvals[-1] = fval


    output_dict = { 'kkts': kkts, 'fvals': fvals, 'fea': feasibility, 'kkt': substationarity, 'fval': fval, 'feas': feas}

    return X, output_dict






def penc_safeguard(X, radius, feas):
    n,p = np.shape(X)

    # r_X = np.sum( X**2 )
    # if  r_X> radius:
    #     X = X / np.sqrt(r_X/ radius)

    if feas > 5e-1 :
        X = np.linalg.solve( X.T @ X + np.eye(p), 2*X.T ).T
    elif feas > 1e-2:
        X = 1.5 * X - 0.5 * X@(X.T @ X) 

    return X 
    