# coding=utf-8
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class RunConfiguration(Model):
    """RunConfiguration.

    :param script:
    :type script: str
    :param command:
    :type command: str
    :param use_absolute_path:
    :type use_absolute_path: bool
    :param arguments:
    :type arguments: list[str]
    :param framework: Possible values include: 'Python', 'PySpark', 'Cntk',
     'TensorFlow', 'PyTorch', 'PySparkInteractive', 'R'
    :type framework: str or ~designer.models.Framework
    :param communicator: Possible values include: 'None', 'ParameterServer',
     'Gloo', 'Mpi', 'Nccl', 'ParallelTask'
    :type communicator: str or ~designer.models.Communicator
    :param target:
    :type target: str
    :param auto_cluster_compute_specification:
    :type auto_cluster_compute_specification:
     ~designer.models.AutoClusterComputeSpecification
    :param data_references:
    :type data_references: dict[str,
     ~designer.models.DataReferenceConfiguration]
    :param data:
    :type data: dict[str, ~designer.models.Data]
    :param output_data:
    :type output_data: dict[str, ~designer.models.OutputData]
    :param datacaches:
    :type datacaches: list[~designer.models.DatacacheConfiguration]
    :param job_name:
    :type job_name: str
    :param max_run_duration_seconds:
    :type max_run_duration_seconds: long
    :param node_count:
    :type node_count: int
    :param instance_types:
    :type instance_types: list[str]
    :param priority:
    :type priority: int
    :param credential_passthrough:
    :type credential_passthrough: bool
    :param identity:
    :type identity: ~designer.models.IdentityConfiguration
    :param environment:
    :type environment: ~designer.models.EnvironmentDefinition
    :param history:
    :type history: ~designer.models.HistoryConfiguration
    :param spark:
    :type spark: ~designer.models.SparkConfiguration
    :param parallel_task:
    :type parallel_task: ~designer.models.ParallelTaskConfiguration
    :param ai_super_computer:
    :type ai_super_computer: ~designer.models.AISuperComputerConfiguration
    :param tensorflow:
    :type tensorflow: ~designer.models.TensorflowConfiguration
    :param mpi:
    :type mpi: ~designer.models.MpiConfiguration
    :param py_torch:
    :type py_torch: ~designer.models.PyTorchConfiguration
    :param hdi:
    :type hdi: ~designer.models.HdiConfiguration
    :param docker:
    :type docker: ~designer.models.DockerConfiguration
    :param command_return_code_config:
    :type command_return_code_config: ~designer.models.CommandReturnCodeConfig
    :param environment_variables:
    :type environment_variables: dict[str, str]
    :param application_endpoints:
    :type application_endpoints: dict[str,
     ~designer.models.ApplicationEndpointConfiguration]
    :param parameters:
    :type parameters: list[~designer.models.ParameterDefinition]
    :param data_bricks:
    :type data_bricks: ~designer.models.DatabricksConfiguration
    """

    _attribute_map = {
        'script': {'key': 'script', 'type': 'str'},
        'command': {'key': 'command', 'type': 'str'},
        'use_absolute_path': {'key': 'useAbsolutePath', 'type': 'bool'},
        'arguments': {'key': 'arguments', 'type': '[str]'},
        'framework': {'key': 'framework', 'type': 'str'},
        'communicator': {'key': 'communicator', 'type': 'str'},
        'target': {'key': 'target', 'type': 'str'},
        'auto_cluster_compute_specification': {'key': 'autoClusterComputeSpecification', 'type': 'AutoClusterComputeSpecification'},
        'data_references': {'key': 'dataReferences', 'type': '{DataReferenceConfiguration}'},
        'data': {'key': 'data', 'type': '{Data}'},
        'output_data': {'key': 'outputData', 'type': '{OutputData}'},
        'datacaches': {'key': 'datacaches', 'type': '[DatacacheConfiguration]'},
        'job_name': {'key': 'jobName', 'type': 'str'},
        'max_run_duration_seconds': {'key': 'maxRunDurationSeconds', 'type': 'long'},
        'node_count': {'key': 'nodeCount', 'type': 'int'},
        'instance_types': {'key': 'instanceTypes', 'type': '[str]'},
        'priority': {'key': 'priority', 'type': 'int'},
        'credential_passthrough': {'key': 'credentialPassthrough', 'type': 'bool'},
        'identity': {'key': 'identity', 'type': 'IdentityConfiguration'},
        'environment': {'key': 'environment', 'type': 'EnvironmentDefinition'},
        'history': {'key': 'history', 'type': 'HistoryConfiguration'},
        'spark': {'key': 'spark', 'type': 'SparkConfiguration'},
        'parallel_task': {'key': 'parallelTask', 'type': 'ParallelTaskConfiguration'},
        'ai_super_computer': {'key': 'aiSuperComputer', 'type': 'AISuperComputerConfiguration'},
        'tensorflow': {'key': 'tensorflow', 'type': 'TensorflowConfiguration'},
        'mpi': {'key': 'mpi', 'type': 'MpiConfiguration'},
        'py_torch': {'key': 'pyTorch', 'type': 'PyTorchConfiguration'},
        'hdi': {'key': 'hdi', 'type': 'HdiConfiguration'},
        'docker': {'key': 'docker', 'type': 'DockerConfiguration'},
        'command_return_code_config': {'key': 'commandReturnCodeConfig', 'type': 'CommandReturnCodeConfig'},
        'environment_variables': {'key': 'environmentVariables', 'type': '{str}'},
        'application_endpoints': {'key': 'applicationEndpoints', 'type': '{ApplicationEndpointConfiguration}'},
        'parameters': {'key': 'parameters', 'type': '[ParameterDefinition]'},
        'data_bricks': {'key': 'dataBricks', 'type': 'DatabricksConfiguration'},
    }

    def __init__(self, *, script: str=None, command: str=None, use_absolute_path: bool=None, arguments=None, framework=None, communicator=None, target: str=None, auto_cluster_compute_specification=None, data_references=None, data=None, output_data=None, datacaches=None, job_name: str=None, max_run_duration_seconds: int=None, node_count: int=None, instance_types=None, priority: int=None, credential_passthrough: bool=None, identity=None, environment=None, history=None, spark=None, parallel_task=None, ai_super_computer=None, tensorflow=None, mpi=None, py_torch=None, hdi=None, docker=None, command_return_code_config=None, environment_variables=None, application_endpoints=None, parameters=None, data_bricks=None, **kwargs) -> None:
        super(RunConfiguration, self).__init__(**kwargs)
        self.script = script
        self.command = command
        self.use_absolute_path = use_absolute_path
        self.arguments = arguments
        self.framework = framework
        self.communicator = communicator
        self.target = target
        self.auto_cluster_compute_specification = auto_cluster_compute_specification
        self.data_references = data_references
        self.data = data
        self.output_data = output_data
        self.datacaches = datacaches
        self.job_name = job_name
        self.max_run_duration_seconds = max_run_duration_seconds
        self.node_count = node_count
        self.instance_types = instance_types
        self.priority = priority
        self.credential_passthrough = credential_passthrough
        self.identity = identity
        self.environment = environment
        self.history = history
        self.spark = spark
        self.parallel_task = parallel_task
        self.ai_super_computer = ai_super_computer
        self.tensorflow = tensorflow
        self.mpi = mpi
        self.py_torch = py_torch
        self.hdi = hdi
        self.docker = docker
        self.command_return_code_config = command_return_code_config
        self.environment_variables = environment_variables
        self.application_endpoints = application_endpoints
        self.parameters = parameters
        self.data_bricks = data_bricks
